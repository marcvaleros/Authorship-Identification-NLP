{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dstack\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "import string\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from random import randrange\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input,Embedding,Dense,LSTM,GRU,Bidirectional,Dropout,SimpleRNN,GlobalAvgPool1D,GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Conv1D,SpatialDropout1D,BatchNormalization,Lambda,Concatenate,concatenate,GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19579 entries, 0 to 19578\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      19579 non-null  object\n",
      " 1   text    19579 non-null  object\n",
      " 2   author  19579 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 459.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removing id column\n",
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19488 entries, 0 to 19578\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    19488 non-null  object\n",
      " 1   author  19488 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 456.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#remove outliers\n",
    "df = df[df['text'].str.split().map(lambda x:len(x))<100]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to preprocess removing punctuations, normalize, stopwords and lemmatization\n",
    "\n",
    "\n",
    "class TextPreprocessing(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 n_jobs=1):    \n",
    "      \n",
    "     self.n_jobs = n_jobs\n",
    "    \"\"\"\n",
    "        Text preprocessing transformer includes steps:\n",
    "            1. Text normalization\n",
    "            2. Punctuation removal\n",
    "            3. Stop words removal\n",
    "            4. Lemmatization\n",
    "        \n",
    "        n_jobs - parallel jobs to run\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        X_copy = X.copy()\n",
    "        partitions = 2\n",
    "        cores = mp.cpu_count()\n",
    "        if self.n_jobs <= -1:\n",
    "          partitions = cores\n",
    "        elif self.n_jobs <= 0:\n",
    "          return X_copy.apply(self._preprocess_text)\n",
    "        else:\n",
    "          partitions = min(self.n_jobs, cores)\n",
    "        cores = mp.cpu_count()\n",
    "        data_split = np.array_split(X_copy, partitions)\n",
    "        pool = mp.Pool(cores)\n",
    "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return data\n",
    "\n",
    "    def _preprocess_part(self, part):\n",
    "        return part.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        normalized_text = self._normalize(text)\n",
    "        doc = nlp(normalized_text)\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "        return self._lemmatize(removed_stop_words)\n",
    "\n",
    "    def _normalize(self, text):\n",
    "        # some issues in normalise package\n",
    "        try:\n",
    "            return ' '.join(normalise(text, verbose=False))\n",
    "        except:\n",
    "            return text\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if not t.is_stop]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        return ' '.join([t.lemma_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  This process, however, afforded me no means of...       0\n",
       "1  It never once occurred to me that the fumbling...       1\n",
       "2  In his left hand was a gold snuff box, from wh...       0\n",
       "3  How lovely is spring As we looked from Windsor...       2\n",
       "4  Finding nothing else, not even gold, the Super...       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the categorical column to variable for easier processing \n",
    "\n",
    "df['author'] = df['author'].map({'EAP':0,'HPL':1,'MWS':2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using countvectorizer to convert the sentence into column of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (19488, 24796)\n",
      "Amount of Non-Zero occurences:  421231\n",
      "Shape of Tfidf Transformed matrix (19488, 24796)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_df = cv.fit_transform(df['text'])\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(cv_df)\n",
    "tfidf_trans = tfidf.transform(cv_df)\n",
    "\n",
    "print('Shape of Sparse Matrix: ', cv_df.shape)\n",
    "print('Amount of Non-Zero occurences: ', cv_df.nnz)\n",
    "print('Shape of Tfidf Transformed matrix',tfidf_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  This process, however, afforded me no means of...       0\n",
       "1  It never once occurred to me that the fumbling...       1\n",
       "2  In his left hand was a gold snuff box, from wh...       0\n",
       "3  How lovely is spring As we looked from Windsor...       2\n",
       "4  Finding nothing else, not even gold, the Super...       1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neural = df\n",
    "df_neural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>process however afford mean ascertain dimensio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>never occur fumble might mere mistake .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>left hand gold snuff box caper hill cut manner...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lovely spring look windsor terrace sixteen fer...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>find nothing else even gold superintendent aba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  process however afford mean ascertain dimensio...       0\n",
       "1            never occur fumble might mere mistake .       1\n",
       "2  left hand gold snuff box caper hill cut manner...       0\n",
       "3  lovely spring look windsor terrace sixteen fer...       2\n",
       "4  find nothing else even gold superintendent aba...       1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def convert_nltk_to_wordnet(text):\n",
    "#To check if the given word is noun,or a verb or an adjective\n",
    "  if text.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  \n",
    "  elif text.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "\n",
    "  elif text.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  \n",
    "  elif text.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  \n",
    "  else:\n",
    "    return None \n",
    " \n",
    "def lemmatizes(sentence):\n",
    "  tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "  wordnet_tagged = map(lambda x : (x[0] , convert_nltk_to_wordnet(x[1])) , tagged)\n",
    "  lemmatized_sentence = []\n",
    "  for word , tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "      lemmatized_sentence.append(word)\n",
    "    else:\n",
    "      lemmatized_sentence.append(lemmatizer.lemmatize(word,tag))\n",
    "  return ' '.join(lemmatized_sentence)\n",
    "\n",
    "def clean(text):\n",
    "\n",
    "  text = re.sub('/.',' ',text)\n",
    "  text = text.lower()\n",
    "  text = re.sub(\"aren't\", \"are not\",text)\n",
    "  text = re.sub(\"can't\",\"cannot\",text)\n",
    "  text = re.sub(\"don't\",\"do not\",text)\n",
    "  text = re.sub(\"couldn't\",\"could not\",text)\n",
    "  text = re.sub(\"doesn't\",\"does not\",text)\n",
    "  text = re.sub(\"hadn't\",\"had not\",text)\n",
    "  text = re.sub(\"wouldn't\",\"would not\",text)\n",
    "  text = re.sub(\"he'll\",\"he will\",text)\n",
    "  text = re.sub(\"what've\",\"what have\",text)\n",
    "  text = re.sub(\"who'd\",\"who would\",text)\n",
    "  text = re.sub(\"who'll\",\"who will\",text)\n",
    "  text = re.sub(\"I'll\",\"I will\",text)\n",
    "  text = re.sub(\"you'd\",\"you would\",text)\n",
    "  text = re.sub(\"you'll\",\"you will\",text)\n",
    "  text = re.sub(\"you're\",\"you are\",text)\n",
    "  text = re.sub(\"you've\",\"you have\",text)\n",
    "  text = re.sub(\"wasn't\",\"was not\",text)\n",
    "  text = re.sub(\"that's\",\"that is\",text)\n",
    "  text = re.sub(\"they've\",\"they have\",text)\n",
    "  text = re.sub(\"they're\",\"they are\",text)\n",
    "  text = re.sub(\"what's\",\"what is\",text)\n",
    "  text = re.sub(\"what're\",\"what are\",text)\n",
    "  text = re.sub(\"what'll\",\"what will\",text)\n",
    "  text = re.sub(\"that's\",\"that is\",text)\n",
    "  text = re.sub(\"there's\",\"there is\",text)\n",
    "  text = re.sub(\"it's\",\"it is\",text)\n",
    "  text = re.sub(\"it'll\",\"it will\",text)\n",
    "  text = re.sub(\"could've\",\"could have\",text)\n",
    "  text = re.sub(\"it'll\",\"it will\",text)\n",
    "  text = re.sub(\"shouldn't\",\"should not\",text)\n",
    "  text = re.sub(\"should've\",\"should have\",text)\n",
    "  text = re.sub(\"shan't\",\"shall not\",text)\n",
    "  text = re.sub(\"won't\",\"will not\",text)\n",
    "  text = re.sub(\"we'd\",\"we would\",text)\n",
    "  text = re.sub(\"weren't\",\"were not\",text)\n",
    "  text = re.sub('[^A-Za-z/.\\s]','',text)\n",
    "  text = text.lower().split()\n",
    "  text = [word for word in text if word not in stop]\n",
    "  text = ' '.join(text)\n",
    "  final_text = lemmatizes(text)\n",
    "  return final_text\n",
    "\n",
    "df_neural['text'] = df_neural['text'].apply(lambda x : clean(x))\n",
    "y = to_categorical(df['author'])\n",
    "df_neural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(name,word_index,vocab_len,dim):\n",
    "  embedding_index = {}\n",
    "  f = open(name,encoding='utf-8')\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coeffs = np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word] = coeffs\n",
    "  f.close()\n",
    "  embedding_matrix = np.zeros((vocab_len+1,dim))\n",
    "  for word,index in word_index.items():\n",
    "    if index > vocab_len:\n",
    "      break\n",
    "    else:\n",
    "      embedding_vector = embedding_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "  return embedding_matrix,embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.asarray(df_neural['text'])\n",
    "tokenizer = Tokenizer(num_words=21000)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "data = pad_sequences(sequences=sequences,padding='pre')\n",
    "vocab_len = len(tokenizer.word_index)+1\n",
    "max_len = len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix1,embedding_index1 = get_embedding('glove.6B.300d.txt',word_index,vocab_len,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we have word embeddings for the words in our vocab\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "\n",
    "  known_words = {}\n",
    "  unknown_words = {}\n",
    "  nb_known_words = 0\n",
    "  nb_unknown_words = 0\n",
    "  for word in vocab.keys():\n",
    "    try:\n",
    "        known_words[word] = embeddings_index[word]\n",
    "        nb_known_words += vocab[word]\n",
    "    except:\n",
    "        unknown_words[word] = vocab[word]\n",
    "        nb_unknown_words += vocab[word]\n",
    "        pass\n",
    "  print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "  print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "  unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "  return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove embeddings:\n",
      "\n",
      "Found embeddings for 87.348% of vocab\n",
      "Found embeddings for  82.289% of all text\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Glove embeddings:\\n')\n",
    "Glove_embedding = check_coverage(word_index,embedding_index1)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brusquerie', 20245),\n",
       " ('tremulousness', 20238),\n",
       " ('aegidus', 20232),\n",
       " ('valentinianus', 20231),\n",
       " ('btenoir', 20227),\n",
       " ('junianus', 20226),\n",
       " ('littlewit', 20224),\n",
       " ('schweinkopf', 20219),\n",
       " ('apothegm', 20216),\n",
       " ('flatzplatz', 20215),\n",
       " ('literatim', 20211),\n",
       " ('odigies', 20209),\n",
       " ('despera', 20208),\n",
       " ('chinless', 20207),\n",
       " ('herbless', 20202),\n",
       " ('trink', 20201),\n",
       " ('deathful', 20199),\n",
       " ('contemns', 20181),\n",
       " ('servox', 20171),\n",
       " ('unpossessed', 20170),\n",
       " ('signalize', 20153),\n",
       " ('carvins', 20149),\n",
       " ('otaheit', 20146),\n",
       " ('miltonic', 20136),\n",
       " ('rayless', 20135),\n",
       " ('siroc', 20129),\n",
       " ('lascia', 20125),\n",
       " ('raggiar', 20124),\n",
       " ('lombra', 20121),\n",
       " ('othair', 20115)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Glove_embedding[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20252, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_matrix_weights = embedding_matrix1\n",
    "np.shape(embedding_matrix_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training ,test and validation set\n",
    "X_train,X_test,y_train,y_test=train_test_split(data,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using BiDirectional LSTM model\n",
    "\n",
    "def deep_1st():\n",
    "  model_deep = Sequential()\n",
    "  model_deep.add(Embedding(vocab_len+1,300,weights=[embedding_matrix_weights],trainable=True,input_length=max_len))\n",
    "  model_deep.add(SpatialDropout1D(0.2))\n",
    "  model_deep.add(Bidirectional(LSTM(128,input_shape=(64,1),return_sequences = True)))\n",
    "  model_deep.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
    "  model_deep.add(GlobalMaxPool1D())\n",
    "  model_deep.add(Dense(128,activation='relu'))\n",
    "  model_deep.add(Dropout(0.5))\n",
    "  model_deep.add(BatchNormalization())\n",
    "  model_deep.add(Dense(3,activation='softmax'))\n",
    "\n",
    "  callbacks = EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "  model_deep.compile(optimizer = 'adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  return model_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of KFold 5\n",
      "Epoch 1/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.4449 - loss: 1.1810 - val_accuracy: 0.4088 - val_loss: 1.0403\n",
      "Epoch 2/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6250 - loss: 0.8416 - val_accuracy: 0.4569 - val_loss: 0.9966\n",
      "Epoch 3/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.7142 - loss: 0.6887 - val_accuracy: 0.5094 - val_loss: 0.9421\n",
      "Epoch 4/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.7652 - loss: 0.5751 - val_accuracy: 0.6405 - val_loss: 0.8785\n",
      "Epoch 5/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8136 - loss: 0.4731 - val_accuracy: 0.7387 - val_loss: 0.7930\n",
      "Epoch 6/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.8590 - loss: 0.3780 - val_accuracy: 0.7487 - val_loss: 0.7264\n",
      "Epoch 7/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8877 - loss: 0.3036 - val_accuracy: 0.7503 - val_loss: 0.6580\n",
      "Epoch 8/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9010 - loss: 0.2590 - val_accuracy: 0.7764 - val_loss: 0.5922\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.7779 - loss: 0.6011\n",
      "Accuracy :   0.7783835530281067\n",
      "\n",
      "\n",
      "2 of KFold 5\n",
      "Epoch 1/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.4269 - loss: 1.1934 - val_accuracy: 0.4329 - val_loss: 1.0372\n",
      "Epoch 2/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6246 - loss: 0.8429 - val_accuracy: 0.5042 - val_loss: 0.9895\n",
      "Epoch 3/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.7089 - loss: 0.7029 - val_accuracy: 0.5150 - val_loss: 0.9494\n",
      "Epoch 4/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.7597 - loss: 0.5802 - val_accuracy: 0.6966 - val_loss: 0.8560\n",
      "Epoch 5/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8213 - loss: 0.4580 - val_accuracy: 0.7134 - val_loss: 0.7895\n",
      "Epoch 6/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.8559 - loss: 0.3820 - val_accuracy: 0.7086 - val_loss: 0.7398\n",
      "Epoch 7/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.8891 - loss: 0.3027 - val_accuracy: 0.7339 - val_loss: 0.6634\n",
      "Epoch 8/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.9042 - loss: 0.2535 - val_accuracy: 0.7756 - val_loss: 0.5839\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.7677 - loss: 0.5992\n",
      "Accuracy :   0.7629891037940979\n",
      "\n",
      "\n",
      "3 of KFold 5\n",
      "Epoch 1/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.4566 - loss: 1.1667 - val_accuracy: 0.4954 - val_loss: 1.0326\n",
      "Epoch 2/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.6333 - loss: 0.8347 - val_accuracy: 0.5523 - val_loss: 0.9892\n",
      "Epoch 3/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.7038 - loss: 0.6891 - val_accuracy: 0.5503 - val_loss: 0.9488\n",
      "Epoch 4/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.7629 - loss: 0.5845 - val_accuracy: 0.6305 - val_loss: 0.8700\n",
      "Epoch 5/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.8078 - loss: 0.4751 - val_accuracy: 0.7186 - val_loss: 0.7906\n",
      "Epoch 6/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.8550 - loss: 0.3721 - val_accuracy: 0.7391 - val_loss: 0.7179\n",
      "Epoch 7/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.8844 - loss: 0.3072 - val_accuracy: 0.7575 - val_loss: 0.6440\n",
      "Epoch 8/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9029 - loss: 0.2628 - val_accuracy: 0.7623 - val_loss: 0.6066\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.7658 - loss: 0.6057\n",
      "Accuracy :   0.7694034576416016\n",
      "\n",
      "\n",
      "4 of KFold 5\n",
      "Epoch 1/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.4411 - loss: 1.1935 - val_accuracy: 0.4866 - val_loss: 1.0462\n",
      "Epoch 2/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.6207 - loss: 0.8573 - val_accuracy: 0.4685 - val_loss: 1.0115\n",
      "Epoch 3/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.6869 - loss: 0.7326 - val_accuracy: 0.5739 - val_loss: 0.9596\n",
      "Epoch 4/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.7649 - loss: 0.5746 - val_accuracy: 0.6561 - val_loss: 0.8858\n",
      "Epoch 5/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.8149 - loss: 0.4741 - val_accuracy: 0.6850 - val_loss: 0.8259\n",
      "Epoch 6/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.8508 - loss: 0.3860 - val_accuracy: 0.7591 - val_loss: 0.7233\n",
      "Epoch 7/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.8758 - loss: 0.3241 - val_accuracy: 0.7615 - val_loss: 0.6642\n",
      "Epoch 8/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9034 - loss: 0.2575 - val_accuracy: 0.7607 - val_loss: 0.6145\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - accuracy: 0.7737 - loss: 0.5996\n",
      "Accuracy :   0.7697241902351379\n",
      "\n",
      "\n",
      "5 of KFold 5\n",
      "Epoch 1/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.4344 - loss: 1.1864 - val_accuracy: 0.5667 - val_loss: 1.0369\n",
      "Epoch 2/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.6228 - loss: 0.8578 - val_accuracy: 0.4565 - val_loss: 1.0038\n",
      "Epoch 3/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.6984 - loss: 0.7102 - val_accuracy: 0.5206 - val_loss: 0.9565\n",
      "Epoch 4/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.7755 - loss: 0.5650 - val_accuracy: 0.5884 - val_loss: 0.9051\n",
      "Epoch 5/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.8164 - loss: 0.4631 - val_accuracy: 0.6826 - val_loss: 0.8240\n",
      "Epoch 6/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.8544 - loss: 0.3733 - val_accuracy: 0.6409 - val_loss: 0.7985\n",
      "Epoch 7/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.8854 - loss: 0.3094 - val_accuracy: 0.7635 - val_loss: 0.6727\n",
      "Epoch 8/8\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.9086 - loss: 0.2489 - val_accuracy: 0.7747 - val_loss: 0.6044\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - accuracy: 0.7670 - loss: 0.6248\n",
      "Accuracy :   0.7758178114891052\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "i=1\n",
    "score=[]\n",
    "for train_index , test_index in kfold.split(X_train,y_train.argmax(1)):\n",
    "    print(f'{i} of KFold {kfold.n_splits}')\n",
    "    X_train1,X_test1 = X_train[train_index],X_train[test_index]\n",
    "    y_train1,y_test1 = y_train[train_index],y_train[test_index]\n",
    "    model_1 = deep_1st()\n",
    "    history = model_1.fit(X_train1,y_train1,batch_size=512,epochs=8,validation_split=0.2)\n",
    "    print('\\n')\n",
    "    acc = model_1.evaluate(X_test1,y_test1)\n",
    "    print('Accuracy :  ',acc[1])\n",
    "    score.append(acc[1])\n",
    "    print('\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.79      1580\n",
      "           1       0.80      0.75      0.77      1113\n",
      "           2       0.85      0.68      0.76      1205\n",
      "\n",
      "    accuracy                           0.78      3898\n",
      "   macro avg       0.79      0.77      0.78      3898\n",
      "weighted avg       0.79      0.78      0.78      3898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model_1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test.argmax(1),pred.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training data set is small , it is preferred to put trainable = False, Let's see what happens if trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_true():\n",
    "\n",
    "  inp = Input(shape=(max_len,))\n",
    "  x = Embedding(vocab_len+1, 300, weights=[embedding_matrix_weights], trainable=True)(inp)\n",
    "  x = SpatialDropout1D(0.3)(x)\n",
    "  x1 = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "  x2 = Bidirectional(GRU(128, return_sequences=True))(x1)\n",
    "  max_pool1 = GlobalMaxPool1D()(x1)\n",
    "  max_pool2 = GlobalMaxPool1D()(x2)\n",
    "  conc = concatenate([max_pool1, max_pool2])\n",
    "  x = Dense(128,activation='relu')(conc)\n",
    "  x = Dropout(0.5)(x)\n",
    "  predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "  model = Model(inputs=inp, outputs=predictions)\n",
    "  callbacks = EarlyStopping(monitor='val_loss',patience=3)\n",
    "\n",
    "  from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "  adam = Adam()\n",
    "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 1s/step - accuracy: 0.4965 - loss: 0.9953 - val_accuracy: 0.6922 - val_loss: 0.7169\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.7339 - loss: 0.6533 - val_accuracy: 0.7475 - val_loss: 0.6100\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - accuracy: 0.8253 - loss: 0.4502 - val_accuracy: 0.7828 - val_loss: 0.5361\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.8756 - loss: 0.3167 - val_accuracy: 0.7904 - val_loss: 0.5446\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 1s/step - accuracy: 0.9133 - loss: 0.2321 - val_accuracy: 0.7876 - val_loss: 0.5796\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 131ms/step - accuracy: 0.7692 - loss: 0.6181\n",
      "Accuracy:   0.7873637080192566\n",
      "2 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - accuracy: 0.5085 - loss: 0.9753 - val_accuracy: 0.6882 - val_loss: 0.7186\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - accuracy: 0.7260 - loss: 0.6685 - val_accuracy: 0.7343 - val_loss: 0.6260\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - accuracy: 0.8230 - loss: 0.4644 - val_accuracy: 0.7884 - val_loss: 0.5363\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.8741 - loss: 0.3352 - val_accuracy: 0.7936 - val_loss: 0.5214\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.9091 - loss: 0.2478 - val_accuracy: 0.7808 - val_loss: 0.6175\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 134ms/step - accuracy: 0.7669 - loss: 0.6372\n",
      "Accuracy:   0.7729313373565674\n",
      "3 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.5178 - loss: 0.9713 - val_accuracy: 0.6758 - val_loss: 0.7564\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 2s/step - accuracy: 0.7238 - loss: 0.6703 - val_accuracy: 0.7571 - val_loss: 0.5852\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 1s/step - accuracy: 0.8146 - loss: 0.4681 - val_accuracy: 0.7856 - val_loss: 0.5557\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.8708 - loss: 0.3475 - val_accuracy: 0.7916 - val_loss: 0.5356\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 2s/step - accuracy: 0.9117 - loss: 0.2450 - val_accuracy: 0.7944 - val_loss: 0.5704\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - accuracy: 0.7894 - loss: 0.6244\n",
      "Accuracy:   0.7902501821517944\n",
      "4 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 2s/step - accuracy: 0.5151 - loss: 0.9730 - val_accuracy: 0.6798 - val_loss: 0.7375\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.7165 - loss: 0.6724 - val_accuracy: 0.7495 - val_loss: 0.5881\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 2s/step - accuracy: 0.8241 - loss: 0.4427 - val_accuracy: 0.7679 - val_loss: 0.5647\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 2s/step - accuracy: 0.8780 - loss: 0.3217 - val_accuracy: 0.7856 - val_loss: 0.5730\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9125 - loss: 0.2192 - val_accuracy: 0.7788 - val_loss: 0.6121\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 175ms/step - accuracy: 0.7926 - loss: 0.5818\n",
      "Accuracy:   0.7867222428321838\n",
      "5 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - accuracy: 0.5094 - loss: 0.9742 - val_accuracy: 0.6906 - val_loss: 0.7204\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2s/step - accuracy: 0.7317 - loss: 0.6500 - val_accuracy: 0.7635 - val_loss: 0.5801\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2s/step - accuracy: 0.8254 - loss: 0.4556 - val_accuracy: 0.7828 - val_loss: 0.5308\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - accuracy: 0.8785 - loss: 0.3285 - val_accuracy: 0.7944 - val_loss: 0.5302\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2s/step - accuracy: 0.9119 - loss: 0.2379 - val_accuracy: 0.8000 - val_loss: 0.5503\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 186ms/step - accuracy: 0.8000 - loss: 0.5549\n",
      "Accuracy:   0.8005131483078003\n"
     ]
    }
   ],
   "source": [
    "kfold=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "score=[]\n",
    "i=1\n",
    "for train_index , test_index in kfold.split(X_train,y_train.argmax(1)):\n",
    "    print(f'{i} of KFold {kfold.n_splits}')\n",
    "    X_train_main,X_val = X_train[train_index],X_train[test_index]\n",
    "    y_train_main,y_val = y_train[train_index],y_train[test_index]\n",
    "    model_2nd = model_true()\n",
    "    history = model_2nd.fit(X_train_main,y_train_main,epochs=5,batch_size=128,validation_split=0.2)\n",
    "    print('\\n')\n",
    "    acc = model_2nd.evaluate(X_val,y_val)\n",
    "    score.append(acc[1])\n",
    "    print('Accuracy:  ',acc[1])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :   0.7875561237335205\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy :  ',np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 254ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      1580\n",
      "           1       0.79      0.78      0.79      1113\n",
      "           2       0.76      0.82      0.79      1205\n",
      "\n",
      "    accuracy                           0.79      3898\n",
      "   macro avg       0.79      0.79      0.79      3898\n",
      "weighted avg       0.79      0.79      0.79      3898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_2 = model_2nd.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test.argmax(1),pred_2.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new complex performs the same as our previous model with an accuracy of 85%, and it seems that the recall of some of the classes are better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3():\n",
    "\n",
    "  sequence_input = Input(shape=(max_len,))\n",
    "  embedding_layer = Embedding(vocab_len+1,300,weights=[embedding_matrix_weights],trainable = True,input_length=max_len)\n",
    "  x = embedding_layer(sequence_input)\n",
    "  x = SpatialDropout1D(0.2)(x)\n",
    "  x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
    "  x = Conv1D(64,kernel_size=2,padding='valid',kernel_initializer=\"he_uniform\")(x)\n",
    "  avg_pool = GlobalAvgPool1D()(x)\n",
    "  max_pool = GlobalMaxPool1D()(x)\n",
    "  x = concatenate([avg_pool,max_pool])\n",
    "  x = Dense(128,activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  pred = Dense(3,activation='softmax')(x)\n",
    "\n",
    "  model_3 = Model(sequence_input, pred)\n",
    "  model_3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  return model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.4158 - loss: 1.0717 - val_accuracy: 0.4557 - val_loss: 1.0372\n",
      "Epoch 2/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.5664 - loss: 0.9310 - val_accuracy: 0.5523 - val_loss: 0.9789\n",
      "Epoch 3/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.5953 - loss: 0.8875 - val_accuracy: 0.6553 - val_loss: 0.7914\n",
      "Epoch 4/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6490 - loss: 0.7997 - val_accuracy: 0.6806 - val_loss: 0.7573\n",
      "Epoch 5/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.6822 - loss: 0.7412 - val_accuracy: 0.6745 - val_loss: 0.7535\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.6911 - loss: 0.7368\n",
      "\n",
      "\n",
      "Accuracy:   0.679602324962616\n",
      "2 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.4383 - loss: 1.0592 - val_accuracy: 0.6012 - val_loss: 0.9230\n",
      "Epoch 2/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.5737 - loss: 0.9552 - val_accuracy: 0.6313 - val_loss: 0.8255\n",
      "Epoch 3/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.6358 - loss: 0.8241 - val_accuracy: 0.6593 - val_loss: 0.7828\n",
      "Epoch 4/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.6704 - loss: 0.7651 - val_accuracy: 0.6673 - val_loss: 0.7577\n",
      "Epoch 5/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.6919 - loss: 0.7302 - val_accuracy: 0.6762 - val_loss: 0.7341\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 81ms/step - accuracy: 0.6800 - loss: 0.7391\n",
      "\n",
      "\n",
      "Accuracy:   0.6815266013145447\n",
      "3 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4260 - loss: 1.0718 - val_accuracy: 0.5363 - val_loss: 0.9449\n",
      "Epoch 2/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.5744 - loss: 0.9199 - val_accuracy: 0.6116 - val_loss: 0.8491\n",
      "Epoch 3/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.6259 - loss: 0.8417 - val_accuracy: 0.6273 - val_loss: 0.8245\n",
      "Epoch 4/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.6639 - loss: 0.7953 - val_accuracy: 0.6589 - val_loss: 0.7649\n",
      "Epoch 5/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.6826 - loss: 0.7529 - val_accuracy: 0.6705 - val_loss: 0.7515\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 86ms/step - accuracy: 0.6721 - loss: 0.7452\n",
      "\n",
      "\n",
      "Accuracy:   0.6709429025650024\n",
      "4 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.4335 - loss: 1.0578 - val_accuracy: 0.5351 - val_loss: 0.9539\n",
      "Epoch 2/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.5663 - loss: 0.9128 - val_accuracy: 0.5804 - val_loss: 0.9529\n",
      "Epoch 3/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.6364 - loss: 0.8336 - val_accuracy: 0.6545 - val_loss: 0.7919\n",
      "Epoch 4/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.6611 - loss: 0.7779 - val_accuracy: 0.6589 - val_loss: 0.7847\n",
      "Epoch 5/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.6796 - loss: 0.7432 - val_accuracy: 0.6609 - val_loss: 0.7678\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 100ms/step - accuracy: 0.6564 - loss: 0.7672\n",
      "\n",
      "\n",
      "Accuracy:   0.6600384712219238\n",
      "5 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.4244 - loss: 1.0678 - val_accuracy: 0.4513 - val_loss: 1.0094\n",
      "Epoch 2/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.5670 - loss: 0.9206 - val_accuracy: 0.5996 - val_loss: 0.8691\n",
      "Epoch 3/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.6334 - loss: 0.8315 - val_accuracy: 0.6501 - val_loss: 0.8091\n",
      "Epoch 4/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.6649 - loss: 0.7803 - val_accuracy: 0.6609 - val_loss: 0.7925\n",
      "Epoch 5/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.6811 - loss: 0.7468 - val_accuracy: 0.6790 - val_loss: 0.7413\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 66ms/step - accuracy: 0.7074 - loss: 0.7116\n",
      "\n",
      "\n",
      "Accuracy:   0.7023733258247375\n"
     ]
    }
   ],
   "source": [
    "kfold=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "i=1\n",
    "score=[]\n",
    "for train_index , test_index in kfold.split(X_train,y_train.argmax(1)):\n",
    "    print(f'{i} of KFold {kfold.n_splits}')\n",
    "    X_train_main,X_val = X_train[train_index],X_train[test_index]\n",
    "    y_train_main,y_val = y_train[train_index],y_train[test_index]\n",
    "    model_3rd = model_3()\n",
    "    history = model_3rd.fit(X_train_main,y_train_main,epochs=5,batch_size=512,validation_split=0.2)\n",
    "    print('\\n')\n",
    "    acc = model_3rd.evaluate(X_val,y_val)\n",
    "    score.append(acc[1])\n",
    "    print('\\n')\n",
    "    print('Accuracy:  ',acc[1])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.72      1580\n",
      "           1       0.74      0.61      0.67      1113\n",
      "           2       0.68      0.71      0.69      1205\n",
      "\n",
      "    accuracy                           0.70      3898\n",
      "   macro avg       0.70      0.69      0.69      3898\n",
      "weighted avg       0.70      0.70      0.70      3898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model_3rd.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test.argmax(1),pred.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4():\n",
    "\n",
    "  inp = Input(shape=(max_len,))\n",
    "  embedding_layer = Embedding(vocab_len+1,300,weights=[embedding_matrix_weights],trainable=True)\n",
    "  x = embedding_layer(inp)\n",
    "  x = SpatialDropout1D(0.2)(x)\n",
    "  x1 = Bidirectional(LSTM(64,return_sequences=True))(x)\n",
    "  x1 = Conv1D(64 ,kernel_size=3,padding='same',activation='linear')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Conv1D(64,kernel_size=3,padding='same',activation='linear')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x2 = Conv1D(64,kernel_size=1,padding='same',activation='linear')(x)\n",
    "  xmain = concatenate([x1,x2])\n",
    "  xmain1 = Conv1D(64,kernel_size=3,padding='same',activation='linear')(xmain)\n",
    "  xmain1 = BatchNormalization()(xmain1)\n",
    "  xmain1 = Conv1D(64,kernel_size=3,padding='same',activation='linear')(xmain1)\n",
    "  xmain1 = BatchNormalization()(xmain1)\n",
    "  x = concatenate([xmain,xmain1])\n",
    "  x = GlobalMaxPool1D()(x)\n",
    "  x = Dense(182,activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = Dense(3,activation = 'softmax')(x)\n",
    "\n",
    "  model_last = Model(inp,x)\n",
    "  model_last.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  return model_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 521ms/step - accuracy: 0.4613 - loss: 1.3522 - val_accuracy: 0.4665 - val_loss: 0.9778\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 546ms/step - accuracy: 0.6568 - loss: 0.8233 - val_accuracy: 0.5651 - val_loss: 0.9025\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 558ms/step - accuracy: 0.7402 - loss: 0.6369 - val_accuracy: 0.6449 - val_loss: 0.8153\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 582ms/step - accuracy: 0.8175 - loss: 0.4565 - val_accuracy: 0.7150 - val_loss: 0.6825\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 542ms/step - accuracy: 0.8617 - loss: 0.3552 - val_accuracy: 0.7403 - val_loss: 0.6520\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.7467 - loss: 0.6205\n",
      "\n",
      "\n",
      "Accuracy:   0.7581782937049866\n",
      "2 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 568ms/step - accuracy: 0.4358 - loss: 1.4392 - val_accuracy: 0.5234 - val_loss: 0.9472\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 548ms/step - accuracy: 0.6342 - loss: 0.8814 - val_accuracy: 0.5467 - val_loss: 0.9148\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 646ms/step - accuracy: 0.7284 - loss: 0.6592 - val_accuracy: 0.6345 - val_loss: 0.7833\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 556ms/step - accuracy: 0.8067 - loss: 0.4772 - val_accuracy: 0.7230 - val_loss: 0.6666\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 524ms/step - accuracy: 0.8692 - loss: 0.3453 - val_accuracy: 0.7495 - val_loss: 0.5870\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.7447 - loss: 0.6123\n",
      "\n",
      "\n",
      "Accuracy:   0.7411802411079407\n",
      "3 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 476ms/step - accuracy: 0.4503 - loss: 1.3924 - val_accuracy: 0.5122 - val_loss: 0.9481\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 434ms/step - accuracy: 0.6355 - loss: 0.8529 - val_accuracy: 0.5118 - val_loss: 0.9716\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 467ms/step - accuracy: 0.7471 - loss: 0.6268 - val_accuracy: 0.5992 - val_loss: 0.8728\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 452ms/step - accuracy: 0.8180 - loss: 0.4640 - val_accuracy: 0.7234 - val_loss: 0.6519\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 475ms/step - accuracy: 0.8745 - loss: 0.3381 - val_accuracy: 0.7611 - val_loss: 0.5816\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.7816 - loss: 0.5524\n",
      "\n",
      "\n",
      "Accuracy:   0.7809493541717529\n",
      "4 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 579ms/step - accuracy: 0.4461 - loss: 1.4355 - val_accuracy: 0.5363 - val_loss: 0.9520\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 525ms/step - accuracy: 0.6305 - loss: 0.8722 - val_accuracy: 0.6365 - val_loss: 0.8361\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 555ms/step - accuracy: 0.7255 - loss: 0.6562 - val_accuracy: 0.6689 - val_loss: 0.7509\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 528ms/step - accuracy: 0.8050 - loss: 0.4960 - val_accuracy: 0.7487 - val_loss: 0.6183\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 546ms/step - accuracy: 0.8515 - loss: 0.3850 - val_accuracy: 0.7535 - val_loss: 0.5788\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7648 - loss: 0.5744\n",
      "\n",
      "\n",
      "Accuracy:   0.7620269656181335\n",
      "5 of KFold 5\n",
      "Epoch 1/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 660ms/step - accuracy: 0.4420 - loss: 1.3557 - val_accuracy: 0.3836 - val_loss: 1.1263\n",
      "Epoch 2/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 632ms/step - accuracy: 0.6527 - loss: 0.8295 - val_accuracy: 0.4822 - val_loss: 0.9732\n",
      "Epoch 3/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 658ms/step - accuracy: 0.7369 - loss: 0.6271 - val_accuracy: 0.5776 - val_loss: 0.9379\n",
      "Epoch 4/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 633ms/step - accuracy: 0.8178 - loss: 0.4653 - val_accuracy: 0.6854 - val_loss: 0.7197\n",
      "Epoch 5/5\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 769ms/step - accuracy: 0.8664 - loss: 0.3535 - val_accuracy: 0.7162 - val_loss: 0.7065\n",
      "\n",
      "\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7136 - loss: 0.6759\n",
      "\n",
      "\n",
      "Accuracy:   0.7081462740898132\n"
     ]
    }
   ],
   "source": [
    "kfold=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "i=1\n",
    "score=[]\n",
    "for train_index , test_index in kfold.split(X_train,y_train.argmax(1)):\n",
    "    print(f'{i} of KFold {kfold.n_splits}')\n",
    "    X_train_main,X_val = X_train[train_index],X_train[test_index]\n",
    "    y_train_main,y_val = y_train[train_index],y_train[test_index]\n",
    "    model_4th = model_4()\n",
    "    history = model_4th.fit(X_train_main,y_train_main,epochs=5,batch_size=128,validation_split=0.2)\n",
    "    print('\\n')\n",
    "    acc = model_4th.evaluate(X_val,y_val)\n",
    "    score.append(acc[1])\n",
    "    print('\\n')\n",
    "    print('Accuracy:  ',acc[1])\n",
    "    i+=1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :   0.7500962257385254\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy :  ' , np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.94      0.75      1580\n",
      "           1       0.91      0.49      0.64      1113\n",
      "           2       0.85      0.63      0.72      1205\n",
      "\n",
      "    accuracy                           0.72      3898\n",
      "   macro avg       0.79      0.69      0.70      3898\n",
      "weighted avg       0.77      0.72      0.71      3898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model_4th.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test.argmax(1),pred.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensembling  (Stacking Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX\n",
    " \n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(stackedX, inputy)\n",
    "\treturn model\n",
    " \n",
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\tyhat = model.predict(stackedX)\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 110ms/step - accuracy: 0.7860 - loss: 0.5942\n",
      "Model Accuracy:  0.778091311454773\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 213ms/step - accuracy: 0.7839 - loss: 0.5870\n",
      "Model Accuracy:  0.7888661026954651\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - accuracy: 0.7123 - loss: 0.6985\n",
      "Model Accuracy:  0.6993330121040344\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.7257 - loss: 0.6737\n",
      "Model Accuracy:  0.7165213227272034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82      1580\n",
      "           1       0.83      0.77      0.80      1113\n",
      "           2       0.81      0.79      0.80      1205\n",
      "\n",
      "    accuracy                           0.81      3898\n",
      "   macro avg       0.81      0.80      0.81      3898\n",
      "weighted avg       0.81      0.81      0.81      3898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "members = [model_1,model_2nd,model_3rd,model_4th]\n",
    "for models in members:\n",
    "  _,acc = models.evaluate(X_test,y_test)\n",
    "  print('Model Accuracy: ', acc)\n",
    " \n",
    "model = fit_stacked_model(members, X_test, y_test.argmax(1))\n",
    "yhat = stacked_prediction(members, model, X_test)\n",
    "\n",
    "print(classification_report(y_test.argmax(1),yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
