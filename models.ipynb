{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dstack\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "import string\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from random import randrange\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input,Embedding,Dense,LSTM,GRU,Bidirectional,Dropout,SimpleRNN,GlobalAvgPool1D,GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Conv1D,SpatialDropout1D,BatchNormalization,Lambda,Concatenate,concatenate,GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Marc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19579 entries, 0 to 19578\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      19579 non-null  object\n",
      " 1   text    19579 non-null  object\n",
      " 2   author  19579 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 459.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removing id column\n",
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19488 entries, 0 to 19578\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    19488 non-null  object\n",
      " 1   author  19488 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 456.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#remove outliers\n",
    "df = df[df['text'].str.split().map(lambda x:len(x))<100]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to preprocess removing punctuations, normalize, stopwords and lemmatization\n",
    "\n",
    "\n",
    "class TextPreprocessing(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 n_jobs=1):    \n",
    "      \n",
    "     self.n_jobs = n_jobs\n",
    "    \"\"\"\n",
    "        Text preprocessing transformer includes steps:\n",
    "            1. Text normalization\n",
    "            2. Punctuation removal\n",
    "            3. Stop words removal\n",
    "            4. Lemmatization\n",
    "        \n",
    "        n_jobs - parallel jobs to run\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        X_copy = X.copy()\n",
    "        partitions = 2\n",
    "        cores = mp.cpu_count()\n",
    "        if self.n_jobs <= -1:\n",
    "          partitions = cores\n",
    "        elif self.n_jobs <= 0:\n",
    "          return X_copy.apply(self._preprocess_text)\n",
    "        else:\n",
    "          partitions = min(self.n_jobs, cores)\n",
    "        cores = mp.cpu_count()\n",
    "        data_split = np.array_split(X_copy, partitions)\n",
    "        pool = mp.Pool(cores)\n",
    "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return data\n",
    "\n",
    "    def _preprocess_part(self, part):\n",
    "        return part.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        normalized_text = self._normalize(text)\n",
    "        doc = nlp(normalized_text)\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "        return self._lemmatize(removed_stop_words)\n",
    "\n",
    "    def _normalize(self, text):\n",
    "        # some issues in normalise package\n",
    "        try:\n",
    "            return ' '.join(normalise(text, verbose=False))\n",
    "        except:\n",
    "            return text\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if not t.is_stop]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        return ' '.join([t.lemma_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  This process, however, afforded me no means of...       0\n",
       "1  It never once occurred to me that the fumbling...       1\n",
       "2  In his left hand was a gold snuff box, from wh...       0\n",
       "3  How lovely is spring As we looked from Windsor...       2\n",
       "4  Finding nothing else, not even gold, the Super...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the categorical column to variable for easier processing \n",
    "\n",
    "df['author'] = df['author'].map({'EAP':0,'HPL':1,'MWS':2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using countvectorizer to convert the sentence into column of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (19488, 24796)\n",
      "Amount of Non-Zero occurences:  421231\n",
      "Shape of Tfidf Transformed matrix (19488, 24796)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_df = cv.fit_transform(df['text'])\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(cv_df)\n",
    "tfidf_trans = tfidf.transform(cv_df)\n",
    "\n",
    "print('Shape of Sparse Matrix: ', cv_df.shape)\n",
    "print('Amount of Non-Zero occurences: ', cv_df.nnz)\n",
    "print('Shape of Tfidf Transformed matrix',tfidf_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the model into train and text split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['text'],df['author'],test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  This process, however, afforded me no means of...       0\n",
       "1  It never once occurred to me that the fumbling...       1\n",
       "2  In his left hand was a gold snuff box, from wh...       0\n",
       "3  How lovely is spring As we looked from Windsor...       2\n",
       "4  Finding nothing else, not even gold, the Super...       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neural = df\n",
    "df_neural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>process however afford mean ascertain dimensio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>never occur fumble might mere mistake .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>left hand gold snuff box caper hill cut manner...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lovely spring look windsor terrace sixteen fer...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>find nothing else even gold superintendent aba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  process however afford mean ascertain dimensio...       0\n",
       "1            never occur fumble might mere mistake .       1\n",
       "2  left hand gold snuff box caper hill cut manner...       0\n",
       "3  lovely spring look windsor terrace sixteen fer...       2\n",
       "4  find nothing else even gold superintendent aba...       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def convert_nltk_to_wordnet(text):\n",
    "#To check if the given word is noun,or a verb or an adjective\n",
    "  if text.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  \n",
    "  elif text.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "\n",
    "  elif text.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  \n",
    "  elif text.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  \n",
    "  else:\n",
    "    return None \n",
    " \n",
    "def lemmatizes(sentence):\n",
    "  tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "  wordnet_tagged = map(lambda x : (x[0] , convert_nltk_to_wordnet(x[1])) , tagged)\n",
    "  lemmatized_sentence = []\n",
    "  for word , tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "      lemmatized_sentence.append(word)\n",
    "    else:\n",
    "      lemmatized_sentence.append(lemmatizer.lemmatize(word,tag))\n",
    "  return ' '.join(lemmatized_sentence)\n",
    "\n",
    "def clean(text):\n",
    "\n",
    "  text = re.sub('/.',' ',text)\n",
    "  text = text.lower()\n",
    "  text = re.sub(\"aren't\", \"are not\",text)\n",
    "  text = re.sub(\"can't\",\"cannot\",text)\n",
    "  text = re.sub(\"don't\",\"do not\",text)\n",
    "  text = re.sub(\"couldn't\",\"could not\",text)\n",
    "  text = re.sub(\"doesn't\",\"does not\",text)\n",
    "  text = re.sub(\"hadn't\",\"had not\",text)\n",
    "  text = re.sub(\"wouldn't\",\"would not\",text)\n",
    "  text = re.sub(\"he'll\",\"he will\",text)\n",
    "  text = re.sub(\"what've\",\"what have\",text)\n",
    "  text = re.sub(\"who'd\",\"who would\",text)\n",
    "  text = re.sub(\"who'll\",\"who will\",text)\n",
    "  text = re.sub(\"I'll\",\"I will\",text)\n",
    "  text = re.sub(\"you'd\",\"you would\",text)\n",
    "  text = re.sub(\"you'll\",\"you will\",text)\n",
    "  text = re.sub(\"you're\",\"you are\",text)\n",
    "  text = re.sub(\"you've\",\"you have\",text)\n",
    "  text = re.sub(\"wasn't\",\"was not\",text)\n",
    "  text = re.sub(\"that's\",\"that is\",text)\n",
    "  text = re.sub(\"they've\",\"they have\",text)\n",
    "  text = re.sub(\"they're\",\"they are\",text)\n",
    "  text = re.sub(\"what's\",\"what is\",text)\n",
    "  text = re.sub(\"what're\",\"what are\",text)\n",
    "  text = re.sub(\"what'll\",\"what will\",text)\n",
    "  text = re.sub(\"that's\",\"that is\",text)\n",
    "  text = re.sub(\"there's\",\"there is\",text)\n",
    "  text = re.sub(\"it's\",\"it is\",text)\n",
    "  text = re.sub(\"it'll\",\"it will\",text)\n",
    "  text = re.sub(\"could've\",\"could have\",text)\n",
    "  text = re.sub(\"it'll\",\"it will\",text)\n",
    "  text = re.sub(\"shouldn't\",\"should not\",text)\n",
    "  text = re.sub(\"should've\",\"should have\",text)\n",
    "  text = re.sub(\"shan't\",\"shall not\",text)\n",
    "  text = re.sub(\"won't\",\"will not\",text)\n",
    "  text = re.sub(\"we'd\",\"we would\",text)\n",
    "  text = re.sub(\"weren't\",\"were not\",text)\n",
    "  text = re.sub('[^A-Za-z/.\\s]','',text)\n",
    "  text = text.lower().split()\n",
    "  text = [word for word in text if word not in stop]\n",
    "  text = ' '.join(text)\n",
    "  final_text = lemmatizes(text)\n",
    "  return final_text\n",
    "\n",
    "df_neural['text'] = df_neural['text'].apply(lambda x : clean(x))\n",
    "y = to_categorical(df['author'])\n",
    "df_neural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(name,word_index,vocab_len,dim):\n",
    "  embedding_index = {}\n",
    "  f = open(name,encoding='utf-8')\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coeffs = np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word] = coeffs\n",
    "  f.close()\n",
    "  embedding_matrix = np.zeros((vocab_len+1,dim))\n",
    "  for word,index in word_index.items():\n",
    "    if index > vocab_len:\n",
    "      break\n",
    "    else:\n",
    "      embedding_vector = embedding_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "  return embedding_matrix,embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.asarray(df_neural['text'])\n",
    "tokenizer = Tokenizer(num_words=21000)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "data = pad_sequences(sequences=sequences,padding='pre')\n",
    "vocab_len = len(tokenizer.word_index)+1\n",
    "max_len = len(data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
